## 前言

在实时渲染中，面片剔除是非常重要的技术。通过把对framebuffer颜色缓冲无贡献的面片在渲染管线front end阶段就丢弃，避免进入back end阶段的光栅化和开销高的片段着色器中，从而极大提升渲染速度，尤其当场景模型数量规模庞大时。例如，在OpenGL里提供了经典的，初学者都能接触到的，隐面剔除api，glEnable(GL_CULL_FACE)，就是通过计算三角形面片法线是否背对观察视线来舍弃背对着观察者的三角形图元。隐面剔除减少了不必要的计算，对于封闭模型的渲染速度有着极大的提升。

但在大规模场景中，光光隐面剔除是不够的。大规模场景中往往伴随着模型间遮挡，仍然存在着大量被遮挡的模型上运行着无意义的着色计算的现象。此时隐面剔除只能确保被遮挡物体的反面不会被渲染，无法避免正面仍进行无效计算。为了解决这个问题，遮挡剔除（occlusion culling）被提了出来。**遮挡剔除通过将被遮挡物体整个丢弃（包含其中成百上千的三角形图元），极大提高了大规模场景的渲染开销。**

不同的是，遮挡剔除的实现方式有很多种。下文将介绍几种常用的方法。

## Occlusion Query

由于各个厂商基本上都在硬件层面提供了遮挡查询（occlusion query）的功能实现，因此OQ是遮挡剔除的最简单的实现方式（除了最古老的early-z rejection之外），且也常常被用来作为衡量其他遮挡剔除算法性能的参照物。

OQ的使用方法非常简单，对于每个要绘制的物体，都要经历如下基本步骤（以OpenGL API举例）：

### 思路

\1. 首先调用glGenQueries()创建一个query对象（OpenGL里用户可以通过query对象获取运行时的数据信息）。

\2. 禁用颜色缓冲写入，避免物体包围盒出现在输出图像上，从而导致错误显示结果。

\3. 禁用深度缓冲写入，但要保持深度测试开启。避免物体包围盒污染深度缓冲中的occluder信息。

\4. 调用glBeginQuery()开启query对象（我们可以把query对象理解成一个计数器，会统计begin和end调用之间所有的相关信息）。注意这里为了告诉OpenGL我们想要query对象专门统计物体的没被遮挡部分的信息，也就是包围盒通过所有测试(视景体，scissor，alpha，stencil和深度测试)的pixel数量信息，我们得传入GL_SAMPLES_PASSED到glBeginQuery()的第一个参数里。

\5. 开始绘制物体的包围盒。

\6. 调用glEndQuery()停止query对象计数。这里我们同样要传入GL_SAMPLES_PASSED。

\7. 调用glGetQueryObjectuiv()获取query对象记录的没被遮挡的包围盒片段数量。

\8. 如果包围盒通过深度测试的片段数量为0（或者低于某一阈值），则可视为物体被遮挡，CPU端就剔除该物体，不传入GPU绘制。

但由于GPU端需要绘制好整个包围盒（当然也有更加激进的策略，例如只绘制包围盒最外围一圈，如果有没被遮挡的部分就算物体通过了遮挡测试）才能知道整个包围盒上到底有多少个片段成功通过了所有的测试，没有被遮挡。因此CPU端会进入stall状态等待GPU，万一GPU管线前面还有一堆非该包围盒的其他geometry没绘制完正在绘制，轮到该包围盒绘制又需要一段时间，CPU端application得陷入stall更久。CPU端application陷入stall对于整个程序绘制性能损失是很大的，因为意味着后面其他的绘制指令都没法被执行（指令和geometry数据没法提前塞入管线，使得管线出现气泡）。

为了缓解这种情况，有好几种策略（有些可以并用）。第一种是尽量把query对象的begin，end操作和查询操作在时间线上间隔越远越好，这样是为了尽可能使得在application查询query数据时结果早已从GPU返回，降低stall的概率。例如先统一绘制一批物体完后，再统一依次查询，这样一来每个query查询的前面都确保经历了足够多的时间。第二种策略是把query结果查询放到下一帧去，也就是说，在下一帧里查询上一帧绘制的query对象。这样基本就能避免CPU陷入stall状态的情况发生。

另外需要注意的是，对于几何形状简单的物体，对它的包围盒遮挡测试反而可能会比直接渲染物体本身开销更大（尤其当屏幕分辨率很高时，包围盒会比原物体占更多的pixel），就算不是如此，也收益甚微。所以对于这些物体最好关闭遮挡测试。遮挡剔除在当原物体几何形状复杂时收益才大。

## HZB-Based Method

HZB的核心思路是生成保存场景深度图的多个低分辨率版本(Depth Hierarchy)。物体距离近，就用分辨率低的深度图测试，距离远，就用高分辨率的深度图测试。这样一来我们就能够在深度测试的性能和精度之前取得平衡，高效地判断一个物体的可见性。

需要注意，虽然生成depth hierarchy过程类似于纹理的mipmap生成，但不同于纹理能够采用线性非线性过滤的下采样方法，深度图是unfiltereable的，需要用shader手动生成。通常生成的texel来源于上一个level对应四个texel中深度值最大的那个（取最大而非其他值，也反映了HZB遮挡剔除的保守特性，不错杀）。实现细节来看，概要地讲就是，首先我们得将物体的三维包围盒投射成屏幕空间上的二维包围盒。

### 思路



Axis-aligned3D包围盒（https://zhuanlan.zhihu.com/p/35321344）的情况是最简单的（若3D包围盒是球体，则投影情况会复杂一些），只要在屏幕空间上取变换后的八个顶点的最大最小的x，y作为二维包围盒的四个顶点，然后取八个顶点上最小的z值代表整个2D包围盒与深度图进行比较。比较开始时，先通过逐层mip level往下查找，直到遇到一层深度图使得四个texel就能容下整个2D包围盒，再进行PCF(percentage closer filtering)深度比较，如下图所示：

![img](https://pic3.zhimg.com/80/v2-f7d6a2a73d53f6643644d6a8e4aadaaa_1440w.webp)

HZB的精髓就在于，通过匹配到恰当的mip level，对于每个物体将深度图查询次数固定在某一值以下(上文中为4次)，对于占屏比例大的物体而言，极大减少了开销和PCF计算量。这样一来，对于屏占比越大的物体，遮挡剔除精度越低（精度低指的是越容易成为漏网之鱼，漏掉剔除，不过没关系，本来屏占比更大的物体更大概率不是被遮挡而是遮挡别人的），反之精度越高。

## Precomputed Visibility

Precomputed visibility（下文简称PV），又叫预计算可见性。优势在不依赖显卡对于遮挡剔除的硬件支持，对显卡要求低，适合移动端。弊端在于更大的内存空间消耗。它较适用于中等规模的场景，因为规模越大，所需要存储在内存里的数据结构就越大，内存吃不消。

### 思路



它将场景中据摄像机的可能活动空间分割成多个cell，每个cell存储着代表从该方格视角看来的场景遮挡信息，通过读取摄像机当前所处的方格对应的预计算好的场景可见性信息来对物体遮挡剔除。这种方法尤其适合环境静态为主，且摄像机的移动受限的应用场景（所需预计算的方格数量就限定住了），例如半开放世界游戏，如下图。

![img](https://pic4.zhimg.com/80/v2-ca8c2b6813824bad89453f5bf7975cef_1440w.webp)

来自UE4关于预计算可见性遮挡剔除部分的官方文档

Cell是在烘焙时借助shadow casting预生成的，后续剔除的时候只需要访问内存数据结构中的遮挡信息即可，开销非常低。而需要注意的是，我们需要在cell颗粒度，占用内存大小和烘培时间之间进行权衡取舍。

PV有几个不足的地方，一个是预生成无法处理场景中的动态遮挡信息，无法剔除动态物体统统把他们当作静态物体对待；二是对透明物体无能为力，全部粗暴地划分为遮挡物和被遮挡物；三是不适合摄像机自由移动的应用场景，那样需要预计算的方格数量太多了。四是场景遮挡信息会一直保存直到程序终止，占据内存时间长；但它预计算的特点，就足以在dynamic occlusion culling面前继续分一杯羹。

## Portal Culling

Portal culling（下文简称PC）先将空间划分成多个cell（虽然预计算可见性也是基于cell，但在PC里，每个cell存储的是与其相邻cells之间的连通性信息），再创建portals联通相邻的cell，如果相邻的两个cell之间没有portal，则这两个cell不连通。Portal类似每个cell之间的窗户，使得处在一个cell中的摄像机透过它能够看到隔壁的另一个cell。PC的原理决定了它特别适用于封闭的户内场景，例如建筑内，山洞内等等，同样也决定了它非常不适合开放的户外场景。

具体来看，PC首先通过定位摄像机所处的cell并对该cell内的所有物体判断是否在视景体内。然后判断该cell内是否有portal位于视景体内，如果有，则借由摄像机位置和portal构造出新的子视景体（包含在原视景体内）并定位子视景体抵达的下一个cell，在下一个cell里递归循环上述操作（直到没有新的子视景体生成）。概略地看，PC通过利用每个cell之间的portal将摄像机初始的视景体层层切分。若物体（bounding box）在切分后的视景体之外，则视为被遮挡并不被渲染（CPU不传入GPU去，即被剔除）。也就是说，在运行时，根据摄像机所处的cell和观察方向，凭借cell之间的联通信息，就可以快速地计算出某一物体是否处于可见范围内。如下图所示：

![img](https://pic1.zhimg.com/80/v2-17af2e20448d5b6e19f11bb35a05cc48_1440w.webp)

来自PANDA3D的portal culling功能说明文档

PC的优点在于可以剔除动态物体，同时允许静态物体比如门（portal）的开关同样影响cell之间的连通性。另外值得一提的是，PC的思想同样可用于对户内场景遮挡光源的剔除，避免错误的光照计算（对于户内静态的物体，甚至可以预计算物体上的被遮挡的静态光源集）。

## Software Occlusion culling

软光栅化隐面剔除，software occlusion culling（下面简称SOC），采用和其他方法完全不同的思路，通过在CPU端的软件内完成geometry的光栅化，从而在软件端直接实现了深度缓冲而不用像原来一样等GPU端生成depth buffer然后传回结果到CPU再判断剔除。为什么好端端的GPU端自动生成的深度缓冲不用，要费尽周折在CPU端自己来直接生成深度缓冲呢？有几点原因：一是GPU端生成的深度缓冲结果传回CPU会有时延开销（latency）。二是如果出现GPU比CPU落后好几帧的情况下，传回的深度缓冲结果更是过时，给CPU的剔除造成视觉显示错误（比如一个当前没被遮挡的物体因为几帧前被遮挡而被错误剔除掉了，导致没显示出来）。三是如果电脑配置是高U低显的话，很容易出现GPU跑满而CPU发呆浪费性能的情况，此时本来GPU就被繁重的物体本身的渲染压得喘不过气，还要分出精力去渲染包围盒用于隐面剔除，确实是我们不想看到的局面。因此SOC在这种情况下就显得非常必要，它把后者的工作移到了CPU端解放了满占用率的GPU，在GPU占用率远比CPU高的情况下提高了整体渲染速度。

虽然CPU的并行处理能力不如GPU，在深度缓冲上光栅化的速度较慢，但我们同样可以通过利用多线程来缩减差距。为了结合多线程，我们将一个深度缓冲分割成多个不相交，紧密排列的区域（tiles），每个区域专门交给一个线程计算。每一帧开始时，先将每个occluder（遮挡物，通常我们会把大型的简单物体，比如墙壁，大门，建筑外墙等在程序运行前事先标记为遮挡物）上的各个三角面片分练放进所处的tile区域的待光栅化队列内（如果一个面片横跨多个tile，则同时放入各个tile内）。然后每个线程开始处理自己的tile区域，对于每个三角形面片，线程将它们光栅化，如果fragment深度值小于深度缓冲保存的pixel深度值，则更新pixel深度值。

生成好occluder深度缓冲之后，我们便在CPU端光栅化occludee（非遮挡物）的包围盒，与深度缓冲比较进行遮挡剔除。如果遮挡剔除通过，CPU才和GPU交互，把geometry和绘制指令输入GPU管线。

SOC也有一些可以完善的地方，比如可以在CPU上于遮挡剔除之前先进行视景体剔除；可以加入SIMD技术配合使用（例如PC上的SSE），应对大量的浮点数计算；可以设置一个occluder屏占尺寸阈值，超过阈值的occluder才会写入occluder深度缓冲，缩短每帧的遮挡剔除深度缓冲生成时间；可以设置一个occludee的屏占尺寸阈值，如果occludee尺寸低于该阈值，则物体就算通过了遮挡深度缓冲测试，也会被舍弃，因为太小了没有绘制的意义。

SOC也有自身的缺点，例如在CPU上光栅化过量的物体，或者过于精细的模型会给CPU带来很大压力，反而拖慢整体渲染速度得不偿失。所以CPU适度接GPU的活，两者良好配合才是最优解。

![img](https://pic1.zhimg.com/80/v2-90c34986da0a36e8918da877bb0927f4_1440w.webp)

来自UE4的软光栅剔除示意图